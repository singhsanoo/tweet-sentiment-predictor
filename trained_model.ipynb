{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47d820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224f642",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d92300",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://uom-twitter-sentiment-analysis.s3.us-east-2.amazonaws.com/Twitter_sentiment_clean.csv\"\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba12b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae3bfe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "def remove_punct(text):\n",
    "    new_text = []\n",
    "    for t in text:\n",
    "        if t not in string.punctuation:\n",
    "            new_text.append(t)\n",
    "    return ''.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d127c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22822512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_sw(text):\n",
    "    new_text = []\n",
    "    for t in text:\n",
    "        if t not in stopwords.words('english'):\n",
    "            new_text.append(t)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce289ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    new_text = []\n",
    "    for t in text:\n",
    "        lem_text = lemmatizer.lemmatize(t)\n",
    "        new_text.append(lem_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = data.iloc[:10000,:]\n",
    "df_1.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f743fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['text'] = df_1['text'].apply(lambda t: remove_punct(t))\n",
    "df_1['text'] = df_1['text'].apply(lambda t: tokenizer.tokenize(t.lower()))\n",
    "df_1['text'] = df_1['text'].apply(lambda t: remove_sw(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda t: remove_punct(t))\n",
    "data['text'] = data['text'].apply(lambda t: tokenizer.tokenize(t.lower()))\n",
    "data['text'] = data['text'].apply(lambda t: remove_sw(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88977880",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda t: word_lemmatizer(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33281ddc",
   "metadata": {},
   "source": [
    "## Split Cleaned Data into Training Set and Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f61c193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['ahhh', 'hope', 'ok']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>['cool', 'tweet', 'apps', 'razr', '2']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>['know', 'family', 'drama', 'lamehey', 'next',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>['school', 'email', 'wont', 'open', 'geography...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>['upper', 'airway', 'problem']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0                             ['ahhh', 'hope', 'ok']\n",
       "1       0             ['cool', 'tweet', 'apps', 'razr', '2']\n",
       "2       0  ['know', 'family', 'drama', 'lamehey', 'next',...\n",
       "3       0  ['school', 'email', 'wont', 'open', 'geography...\n",
       "4       0                     ['upper', 'airway', 'problem']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read preprocessed csv\n",
    "url = \"https://uom-twitter-sentiment-analysis.s3.us-east-2.amazonaws.com/Lemmatize.csv\"\n",
    "lemmatized_df = pd.read_csv(url)\n",
    "lemmatized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ff9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lemmatized_df['text']\n",
    "y = lemmatized_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7e66abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TfidV = TfidfVectorizer()\n",
    "X = TfidV.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ecac883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b94296",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "105fff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes Model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nbc = MultinomialNB()\n",
    "nbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155734cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_nbc = nbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1c2839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.760176194374941\n",
      "Training Score: 0.8103716666666667\n",
      "Testing Score: 0.7649325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "cm_nbc = confusion_matrix(y_test, y_predict_nbc)\n",
    "\n",
    "f1_nbc = f1_score(y_test, y_predict_nbc)\n",
    "print(f'F1 Score: {f1_nbc}')\n",
    "\n",
    "training_score = nbc.score(X_train, y_train)\n",
    "print(f'Training Score: {training_score}')\n",
    "\n",
    "testing_score = nbc.score(X_test, y_test)\n",
    "print(f'Testing Score: {testing_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "220fa3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[156953,  43020],\n",
       "       [ 51007, 149020]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print confusion matrix\n",
    "cm_nbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bbc181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing nbc model for best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'alpha': [1, 0.1, 0.001, 0.0001, 0.00001, 0.000001], 'fit_prior' :[True, False]}\n",
    "gs_nbc = GridSearchCV(nbc, parameters, n_jobs = -1)\n",
    "gs_nbc = gs_nbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04b788f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76318"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_nbc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33fed22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1, 'fit_prior': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_nbc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976d3d4",
   "metadata": {},
   "source": [
    "## SGD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c30e65b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1e-06)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD Model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(alpha = 1e-06, penalty = 'l2')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c327c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_clf = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199d3d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7877500416531907\n",
      "Training Score: 0.8198575\n",
      "Testing Score: 0.783435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "cm_clf = confusion_matrix(y_test, y_predict_clf)\n",
    "\n",
    "f1_clf = f1_score(y_test, y_predict_clf)\n",
    "print(f'F1 Score: {f1_clf}')\n",
    "\n",
    "training_score = clf.score(X_train, y_train)\n",
    "print(f'Training Score: {training_score}')\n",
    "\n",
    "testing_score = clf.score(X_test, y_test)\n",
    "print(f'Testing Score: {testing_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing clf model for best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'alpha': [1, 0.1, 0.001, 0.0001, 0.00001, 0.000001], 'fit_prior' :[True, False], 'class_prior': ['optimal'], 'loss': ['log_loss', 'hinge', 'perceptron', 'modified_huber']}\n",
    "gs_clf = GridSearchCV(nbc, parameters, n_jobs = -1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76766f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a7e45",
   "metadata": {},
   "source": [
    "## Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45dd146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonkwiatkowski/opt/anaconda3/envs/PythonData38/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lrc = LogisticRegression()\n",
    "lrc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "859eeff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_lrc = lrc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "049020d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7883377878540611\n",
      "Training Score: 0.7958616666666667\n",
      "Testing Score: 0.783405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "cm_lrc = confusion_matrix(y_test, y_predict_lrc)\n",
    "\n",
    "f1_lrc = f1_score(y_test, y_predict_lrc)\n",
    "print(f'F1 Score: {f1_lrc}')\n",
    "\n",
    "training_score = lrc.score(X_train, y_train)\n",
    "print(f'Training Score: {training_score}')\n",
    "\n",
    "testing_score = lrc.score(X_test, y_test)\n",
    "print(f'Testing Score: {testing_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\"penalty\":[\"l1\",\"l2\"], 'C': np.logspace(-3,3,7), 'solver'  : ['newton-cg', 'lbfgs', 'liblinear']}\n",
    "gs_lrc = GridSearchCV(lrc, parameters)\n",
    "gs_lrc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lrc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7123b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lrc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556cf719",
   "metadata": {},
   "source": [
    "## LGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a9bc10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(scale_pos_weight=3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LGB Model\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb = LGBMClassifier(scale_pos_weight = 3)\n",
    "lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7c609e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_lgb = lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0445c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7380586789454163\n",
      "Training Score: 0.6591325\n",
      "Testing Score: 0.6592425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "cm_lgb = confusion_matrix(y_test, y_predict_lgb)\n",
    "\n",
    "f1_lgb = f1_score(y_test, y_predict_lgb)\n",
    "print(f'F1 Score: {f1_lgb}')\n",
    "\n",
    "training_score = lgb.score(X_train, y_train)\n",
    "print(f'Training Score: {training_score}')\n",
    "\n",
    "testing_score = lgb.score(X_test, y_test)\n",
    "print(f'Testing Score: {testing_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f83af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing lgb model for best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'num_leaves': [32, 31, 30, 29, 28], 'max_depth' :[-1, -2, -3, -4, -5, -6], 'n_estimators': [100, 200, 500, 1000], 'learning_rate': [.1, .25, .5, .01, .001]}\n",
    "gs_lgb = GridSearchCV(lgb, parameters, n_jobs=-1, verbose=2)\n",
    "gs_lgb = gs_lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e789f1",
   "metadata": {},
   "source": [
    "## Saving the Vectorizer and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d13fac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the vectorizer\n"
     ]
    }
   ],
   "source": [
    "# Save the scaling function to a pickle file (i.e., \"pickle it\")\n",
    "# so we can use it from the Flask server. \n",
    "print('Saving the vectorizer')\n",
    "dump(TfidV, open('TfidV.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26d147a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model\n"
     ]
    }
   ],
   "source": [
    "# Save the model to a pickle file (i.e., \"pickle it\")\n",
    "# so we can use it from the Flask server. \n",
    "print('Saving the model')\n",
    "dump(clf, open('clf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c50d55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model\n"
     ]
    }
   ],
   "source": [
    "# Save the model to a pickle file (i.e., \"pickle it\")\n",
    "# so we can use it from the Flask server. \n",
    "print('Saving the model')\n",
    "dump(lrc, open('lrc.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80cbb41",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d69cfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction labels.\n",
    "predict_labels = ['Negative', 'Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef44db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "clf1 = load(open('clf.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66880d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc1 = load(open('lrc.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28caadf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vectorizer.\n",
    "TfidV1 = load(open('TfidV.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01318d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create three diffferent sets of inputs (i.e., three\n",
    "# different irises). Note that each set is constructed \n",
    "# as a list inside of another list (or an array inside of\n",
    "# another array). This is how scikit-learn needs it. \n",
    "input_row = ['Minnesota drivers are fantastic!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2806d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Tokenizing etc\n",
    "input_row = remove_punct(input_row)\n",
    "input_row = tokenizer.tokenize(input_row.lower())\n",
    "input_row = remove_sw(input_row)\n",
    "input_row = word_lemmatizer(input_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00e2a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Transform each input using the scaler function.\n",
    "input_row_vectorized = TfidV1.transform(input_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e68f7509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions: \n",
      "> Prediction: Positive\n"
     ]
    }
   ],
   "source": [
    "# 4. Make a prediction for each input.\n",
    "print('Making predictions: ')\n",
    "predict = lrc1.predict(input_row1_vectorized)\n",
    "print(f'> Prediction: {predict_labels[predict[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b252c16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tweet is predicted to be 41.0% negative.\n",
      "This tweet is predicted to be 59.0% positive.\n"
     ]
    }
   ],
   "source": [
    "prob = np.round(lrc1.predict_proba(input_row_vectorized)[0], 2) * 100\n",
    "print(f'This tweet is predicted to be {prob[0]}% negative.')\n",
    "print(f'This tweet is predicted to be {prob[1]}% positive.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "748a1c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f781d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
