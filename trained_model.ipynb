{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47d820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224f642",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d92300",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://uom-twitter-sentiment-analysis.s3.us-east-2.amazonaws.com/Twitter_sentiment_clean.csv\"\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba12b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae3bfe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "def remove_punct(text):\n",
    "    new_text = []\n",
    "    for t in text:\n",
    "        if t not in string.punctuation:\n",
    "            new_text.append(t)\n",
    "    return ''.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d127c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22822512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_sw(text):\n",
    "    new_text = []\n",
    "    for t in text:\n",
    "        if t not in stopwords.words('english'):\n",
    "            new_text.append(t)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce289ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    new_text = []\n",
    "    for t in text:\n",
    "        lem_text = lemmatizer.lemmatize(t)\n",
    "        new_text.append(lem_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1c93c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AHHH I HOPE YOUR OK!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>cool , i have no tweet apps  for my razr 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i know  just family drama. its lame.hey next ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>School email won't open  and I have geography ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>upper airways problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Going to miss Pastor's sermon on Faith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>on lunch....dj should come eat with me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>oh why are you feeling like that?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>gahh noo!peyton needs to live!this is horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>thank you glad you like it! There is a produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Zach makes me pee sitting down! And I'm a gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>to sum up my day in one word ......... kackered!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Great minds think alike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>Is Poorly and in bed!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh, that's really great  Here we have a small...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>lol, calm down.  i got a 30day loan offer for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm feeling quite sleepy today, wish i could s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>Yeah  Mathieu totally choked in the 3rd set t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>ugh, morning's off to a rough start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>just bit my tongue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>Apparently you dont have time for ur fans!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>Whaddup Whaddup Whaddup Whaddup Whaddup  I Got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>I'd be willing to pay $40-50 (thus covering t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>Working on photos from Hillsong's 1 year celeb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>baby's chicken pox are soo bad on the face, I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>i don't feel good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>ha ha I want to see pictures from when you we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>This day will pass slowly.  All of these days ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>my car is poorly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>im turning 18 one week from now  but i don't f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>No woooooork tomorrrow&amp;amp;tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>Flatmates are still in the bathroom! ARGGH!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm having an allergic reaction to silver eari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>I am, I'll admit...  maybe I'll feel better t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>Going back to sleep is such a great idea, unf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>what is with msn making me click the wrong peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>Too full of fizz but not enough cider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>How to Get Your Ex Back (worked for me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>Time to sleep... Long day again tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>Hahaha blakk is reckkkin it. :p throwwwwwed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>Cubs game today! My first ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>~ hi! sounds like you're feeling a bit better...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>LOL.  Leave a kid on internet and the kid wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>eating cha with eric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>On a good note, we DID meet at Marty Richards...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>My hands are officially tied. I don't think I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>I said &amp;quot;scar,&amp;quot; sweetie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>it was supposed to go live on tv but it didn'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>FCK-Brï¿½ndby 4-0!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>jealous!  I'm yet to find a local chinese i'm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target                                               text\n",
       "0        0                            AHHH I HOPE YOUR OK!!! \n",
       "1        0         cool , i have no tweet apps  for my razr 2\n",
       "2        0   i know  just family drama. its lame.hey next ...\n",
       "3        0  School email won't open  and I have geography ...\n",
       "4        0                             upper airways problem \n",
       "5        0         Going to miss Pastor's sermon on Faith... \n",
       "6        1            on lunch....dj should come eat with me \n",
       "7        0                 oh why are you feeling like that? \n",
       "8        0    gahh noo!peyton needs to live!this is horrible \n",
       "9        1   thank you glad you like it! There is a produc...\n",
       "10       1   Zach makes me pee sitting down! And I'm a gro...\n",
       "11       1  to sum up my day in one word ......... kackered! \n",
       "12       1                           Great minds think alike \n",
       "13       0                             Is Poorly and in bed! \n",
       "14       1   Oh, that's really great  Here we have a small...\n",
       "15       1   lol, calm down.  i got a 30day loan offer for...\n",
       "16       1  i'm feeling quite sleepy today, wish i could s...\n",
       "17       0   Yeah  Mathieu totally choked in the 3rd set t...\n",
       "18       0               ugh, morning's off to a rough start \n",
       "19       0                                just bit my tongue \n",
       "20       0   Apparently you dont have time for ur fans!!!!...\n",
       "21       1  Whaddup Whaddup Whaddup Whaddup Whaddup  I Got...\n",
       "22       1   I'd be willing to pay $40-50 (thus covering t...\n",
       "23       1  Working on photos from Hillsong's 1 year celeb...\n",
       "24       0  baby's chicken pox are soo bad on the face, I ...\n",
       "25       0                                 i don't feel good \n",
       "26       1   ha ha I want to see pictures from when you we...\n",
       "27       0  This day will pass slowly.  All of these days ...\n",
       "28       0                                  my car is poorly \n",
       "29       1  im turning 18 one week from now  but i don't f...\n",
       "30       1                No woooooork tomorrrow&amp;tuesday \n",
       "31       0      Flatmates are still in the bathroom! ARGGH!! \n",
       "32       0  I'm having an allergic reaction to silver eari...\n",
       "33       0   I am, I'll admit...  maybe I'll feel better t...\n",
       "34       0   Going back to sleep is such a great idea, unf...\n",
       "35       0  what is with msn making me click the wrong peo...\n",
       "36       0             Too full of fizz but not enough cider \n",
       "37       1           How to Get Your Ex Back (worked for me  \n",
       "38       0          Time to sleep... Long day again tomorrow \n",
       "39       1      Hahaha blakk is reckkkin it. :p throwwwwwed. \n",
       "40       1                    Cubs game today! My first ever \n",
       "41       1   ~ hi! sounds like you're feeling a bit better...\n",
       "42       1   LOL.  Leave a kid on internet and the kid wil...\n",
       "43       1                              eating cha with eric \n",
       "44       1   On a good note, we DID meet at Marty Richards...\n",
       "45       0  My hands are officially tied. I don't think I'...\n",
       "46       1                 I said &quot;scar,&quot; sweetie. \n",
       "47       0   it was supposed to go live on tv but it didn'...\n",
       "48       1                                FCK-Brï¿½ndby 4-0! \n",
       "49       0   jealous!  I'm yet to find a local chinese i'm..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = data.iloc[:10000,:]\n",
    "df_1.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f743fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['text'] = df_1['text'].apply(lambda t: remove_punct(t))\n",
    "df_1['text'] = df_1['text'].apply(lambda t: tokenizer.tokenize(t.lower()))\n",
    "df_1['text'] = df_1['text'].apply(lambda t: remove_sw(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda t: remove_punct(t))\n",
    "data['text'] = data['text'].apply(lambda t: tokenizer.tokenize(t.lower()))\n",
    "data['text'] = data['text'].apply(lambda t: remove_sw(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88977880",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda t: word_lemmatizer(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33281ddc",
   "metadata": {},
   "source": [
    "## Split Cleaned Data into Training Set and Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f61c193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['ahhh', 'hope', 'ok']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>['cool', 'tweet', 'apps', 'razr', '2']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>['know', 'family', 'drama', 'lamehey', 'next',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>['school', 'email', 'wont', 'open', 'geography...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>['upper', 'airway', 'problem']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0                             ['ahhh', 'hope', 'ok']\n",
       "1       0             ['cool', 'tweet', 'apps', 'razr', '2']\n",
       "2       0  ['know', 'family', 'drama', 'lamehey', 'next',...\n",
       "3       0  ['school', 'email', 'wont', 'open', 'geography...\n",
       "4       0                     ['upper', 'airway', 'problem']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read preprocessed csv\n",
    "url = \"https://uom-twitter-sentiment-analysis.s3.us-east-2.amazonaws.com/Lemmatize.csv\"\n",
    "lemmatized_df = pd.read_csv(url)\n",
    "lemmatized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87ff9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lemmatized_df['text']\n",
    "y = lemmatized_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7e66abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TfidV = TfidfVectorizer()\n",
    "X = TfidV.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ecac883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b94296",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "105fff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes Model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nbc = MultinomialNB()\n",
    "nbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "155734cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_nbc = nbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd1c2839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.760176194374941\n",
      "Training Score: 0.8103716666666667\n",
      "Testing Score: 0.7649325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "cm_nbc = confusion_matrix(y_test, y_predict_nbc)\n",
    "\n",
    "f1_nbc = f1_score(y_test, y_predict_nbc)\n",
    "print(f'F1 Score: {f1_nbc}')\n",
    "\n",
    "training_score = nbc.score(X_train, y_train)\n",
    "print(f'Training Score: {training_score}')\n",
    "\n",
    "testing_score = nbc.score(X_test, y_test)\n",
    "print(f'Testing Score: {testing_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,

   "id": "220fa3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[156953,  43020],\n",
       "       [ 51007, 149020]])"
      ]
     },

     "execution_count": 9,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print confusion matrix\n",
    "cm_nbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bbc181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing nbc model for best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'alpha': [1, 0.1, 0.001, 0.0001, 0.00001, 0.000001], 'fit_prior' :[True, False]}\n",
    "gs_nbc = GridSearchCV(nbc, parameters, n_jobs = -1)\n",
    "gs_nbc = gs_nbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04b788f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76318"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_nbc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33fed22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1, 'fit_prior': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_nbc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976d3d4",
   "metadata": {},
   "source": [
    "## SGD Model"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 13,

   "id": "c30e65b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1e-06)"
      ]
     },

     "execution_count": 13,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD Model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(alpha = 1e-06, penalty = 'l2')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 14,

   "id": "c327c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_clf = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 15,

   "id": "199d3d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

      "F1 Score: 0.7877500416531907\n",
      "Training Score: 0.8198575\n",
      "Testing Score: 0.783435\n"

     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "cm_clf = confusion_matrix(y_test, y_predict_clf)\n",
    "\n",
    "f1_clf = f1_score(y_test, y_predict_clf)\n",
    "print(f'F1 Score: {f1_clf}')\n",
    "\n",
    "training_score = clf.score(X_train, y_train)\n",
    "print(f'Training Score: {training_score}')\n",
    "\n",
    "testing_score = clf.score(X_test, y_test)\n",
    "print(f'Testing Score: {testing_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing clf model for best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'alpha': [1, 0.1, 0.001, 0.0001, 0.00001, 0.000001], 'fit_prior' :[True, False], 'class_prior': ['optimal'], 'loss': ['log_loss', 'hinge', 'perceptron', 'modified_huber']}\n",
    "gs_clf = GridSearchCV(nbc, parameters, n_jobs = -1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76766f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a7e45",
   "metadata": {},
   "source": [
    "## Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45dd146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongroenewold/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lrc = LogisticRegression()\n",
    "lrc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "859eeff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_lrc = lrc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "049020d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7883377878540611\n",
      "Training Score: 0.7958616666666667\n",
      "Testing Score: 0.783405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "cm_lrc = confusion_matrix(y_test, y_predict_lrc)\n",
    "\n",
    "f1_lrc = f1_score(y_test, y_predict_lrc)\n",
    "print(f'F1 Score: {f1_lrc}')\n",
    "\n",
    "training_score = lrc.score(X_train, y_train)\n",
    "print(f'Training Score: {training_score}')\n",
    "\n",
    "testing_score = lrc.score(X_test, y_test)\n",
    "print(f'Testing Score: {testing_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\"penalty\":[\"l1\",\"l2\"], 'C': np.logspace(-3,3,7), 'solver'  : ['newton-cg', 'lbfgs', 'liblinear']}\n",
    "gs_lrc = GridSearchCV(lrc, parameters)\n",
    "gs_lrc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lrc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7123b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lrc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556cf719",
   "metadata": {},
   "source": [
    "## LGB Model"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 19,
   "id": "2a9bc10f",

   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(scale_pos_weight=3)"
      ]
     },

     "execution_count": 19,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LGB Model\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb = LGBMClassifier(scale_pos_weight = 3)\n",
    "lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 20,
   "id": "b7c609e2",

   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_lgb = lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 21,
   "id": "e0445c1f",

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7380586789454163\n",
      "Training Score: 0.6591325\n",
      "Testing Score: 0.6592425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "cm_lgb = confusion_matrix(y_test, y_predict_lgb)\n",
    "\n",
    "f1_lgb = f1_score(y_test, y_predict_lgb)\n",
    "print(f'F1 Score: {f1_lgb}')\n",
    "\n",
    "training_score = lgb.score(X_train, y_train)\n",
    "print(f'Training Score: {training_score}')\n",
    "\n",
    "testing_score = lgb.score(X_test, y_test)\n",
    "print(f'Testing Score: {testing_score}')"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "e4f83af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing lgb model for best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'num_leaves': [32, 31, 30, 29, 28], 'max_depth' :[-1, -2, -3, -4, -5, -6], 'n_estimators': [100, 200, 500, 1000], 'learning_rate': [.1, .25, .5, .01, .001]}\n",
    "gs_lgb = GridSearchCV(lgb, parameters, n_jobs=-1, verbose=2)\n",
    "gs_lgb = gs_lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e789f1",
   "metadata": {},
   "source": [
    "## Saving the Vectorizer and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d13fac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the vectorizer\n"
     ]
    }
   ],
   "source": [
    "# Save the scaling function to a pickle file (i.e., \"pickle it\")\n",
    "# so we can use it from the Flask server. \n",
    "print('Saving the vectorizer')\n",
    "dump(TfidV, open('TfidV.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26d147a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model\n"
     ]
    }
   ],
   "source": [
    "# Save the model to a pickle file (i.e., \"pickle it\")\n",
    "# so we can use it from the Flask server. \n",
    "print('Saving the model')\n",
    "dump(clf, open('clf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c50d55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model\n"
     ]
    }
   ],
   "source": [
    "# Save the model to a pickle file (i.e., \"pickle it\")\n",
    "# so we can use it from the Flask server. \n",
    "print('Saving the model')\n",
    "dump(lrc, open('lrc.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80cbb41",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d69cfee1",

   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      4\u001b[0m rfc \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mrfc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    439\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    442\u001b[0m ]\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    183\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 185\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m ):\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    411\u001b[0m         splitter,\n\u001b[1;32m    412\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    418\u001b[0m     )\n\u001b[0;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define prediction labels.\n",
    "predict_labels = ['Negative', 'Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef44db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "clf1 = load(open('clf.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66880d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc1 = load(open('lrc.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 28,
   "id": "28caadf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vectorizer.\n",
    "TfidV1 = load(open('TfidV.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01318d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create three diffferent sets of inputs (i.e., three\n",
    "# different irises). Note that each set is constructed \n",
    "# as a list inside of another list (or an array inside of\n",
    "# another array). This is how scikit-learn needs it. \n",
    "input_row = ['Minnesota drivers are fantastic!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2806d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Tokenizing etc\n",
    "input_row = remove_punct(input_row)\n",
    "input_row = tokenizer.tokenize(input_row.lower())\n",
    "input_row = remove_sw(input_row)\n",
    "input_row = word_lemmatizer(input_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00e2a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Transform each input using the scaler function.\n",
    "input_row_vectorized = TfidV1.transform(input_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e68f7509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions: \n",
      "> Prediction: Positive\n"
     ]
    }
   ],

   "source": [
    "# 4. Make a prediction for each input.\n",
    "print('Making predictions: ')\n",
    "predict = lrc1.predict(input_row1_vectorized)\n",
    "print(f'> Prediction: {predict_labels[predict[0]]}')"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 53,
   "id": "b252c16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tweet is predicted to be 41.0% negative.\n",
      "This tweet is predicted to be 59.0% positive.\n"
     ]
    }
   ],
   "source": [
    "prob = np.round(lrc1.predict_proba(input_row_vectorized)[0], 2) * 100\n",
    "print(f'This tweet is predicted to be {prob[0]}% negative.')\n",
    "print(f'This tweet is predicted to be {prob[1]}% positive.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "748a1c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "6f781d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
