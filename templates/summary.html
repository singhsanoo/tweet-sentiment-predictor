<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <title>Alexis – Photography Responsive Bootstrap 5 Template</title>

    <!--== Favicon ==-->
    <link rel="shortcut icon" href="static/img/favicon.png" type="image/x-icon" />

    <!--== Main Style CSS ==-->
    <link href="static/style.css" rel="stylesheet" />

</head>

<body>

<!--wrapper start-->
<div class="wrapper portfolio-single-wrapper">

  <!--== Start Preloader Content ==-->
  <div class="preloader-wrap">
    <div class="preloader">
      <span class="dot"></span>
      <div class="dots">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </div>
  </div>
  <!--== End Preloader Content ==-->

  <!--== Start Header Wrapper ==-->
  <header class="header-area header-default sticky-header hb-border">
    <div class="container-fluid">
      <div class="row align-items-center justify-content-between">
        <!-- Hamburger Menu Start -->
         <!-- Hamburger Menu Start -->
      <div class="col-auto">
        <div class="header-navigation-area d-none">
          <ul class="main-menu nav justify-content-center">
            <li><a href="{{ url_for('home') }}">Home</a></li>
            <li><a href="{{ url_for('team') }}">About</a></li>
            <li><a href="{{ url_for('word') }}">Word Collage</a></li>
            <li><a href="#">Summary</a></li>
          </ul>
        </div>
        <div class="header-action-area">
          <button class="btn-menu">
            <span></span>
            <span></span>
            <span></span>
          </button>
          <span class="menu-text">Menu</span>
        </div>
      </div>
      <!-- Hamburger Menu Start -->

        <!-- Header Logo Start -->
        <div class="col-auto" id = "parentDiv">
          <div class="header-logo-area" id = "mainHeader">
            <a href="{{ url_for('home') }}">
              <img class="logo-main" src="static/img/sentiment.png" alt="Logo" />
              <img class="logo-light" src="static/img/sentiment.png" alt="Logo" />
            </a>
          </div>
        </div>
        <!-- Header Logo End -->

        <!-- Header Search Start -->
        <div class="col-auto">
        </div>
        <!-- Header Search Start -->
      </div>
    </div>
  </header>
  <!--== End Header Wrapper ==-->
  
  <main class="main-content site-wrapper-reveal bgcolor-f6">
    <!--== Start Blog Area Wrapper ==-->
    <div class="blog-details-area">
      <!--== Start Post Details Content ==-->
      <div class="post-details-content">
        <div class="swiper-container post-gallery">
          <div class="swiper-wrapper gallery-slider">
          </div>
          <!-- Add Pagination -->
          <div class="swiper-pagination"></div>
        </div>
        <div class="post-details-body">
          <div class="bread-crumbs">
          </div>
          <div class="content">
            <h2 class="title">Model Descriptions</h2>

            <h3 style = color:blue;>Stochastic Gradient Descent </h3> <p>Gradient descent is an optimization algorithm that attempts to minimize a cost function. However, each iteration of gradient descent runs through the entire datasets searching for an optimal solution. Stochastic gradient descent  makes this process a bit faster. It shuffles the data to avoid any pre-existing patterns, divides it into a finite number of subsets. Each iteration is then performed on an individual subset rather than the entire dataset. This makes  regular gradient descent but can result in a higher variance. </p>

            <h3 style = color:blue;>Light Gradient Boosting </h3> <p> Gradient boosting is a model based on decision trees. However, unlike Random Forests where trees are built independently, gradient boosted trees are built additively. That is, they correct the errors of the trees that came before. This leads to faster training times, lower memory usage, and higher accuracy working with complex data. This makes LGB ideal for very large data sets. However, they are far more susceptible to noise. </p> 
              
            <h3 style = color:blue;>Logistic Regression </h3> <p> Logistic regression is an algorithm that classifies data by considering extreme outcomes and tries to make a logarithmic line that distinguishes between them. Logistic models are simple and provide great efficiency. However, they’re not as flexible as other models so it’s difficult for them to capture more complex relationships.</p>
              
            <h3 style = color:blue;>Multinomial Naive Bayes</h3> <p>Naive Bayes classifier makes use of Bayes’s Theorem which describes the relationship between conditional probabilities  of certain quantities. This lets us find the probability of a label given some observed features. Multinomial Naive Bayes takes this a step further by analyzing how often a given term is represented. This model is well-suited for large data sets because it is so fast. The biggest downfall of this model is known as the zero-frequency problem. That is, it assigns a probability of zero to a categorical value that is present in the testing set but wasn’t available in the training set. </p> 

            <h2 class="title">Project Summary</h2>
            <p>The goal of this project was to see if it was possible to train a machine learning model to recognize the sentiment of a tweet. We began by discussing possible limitations. In the end, we wanted to host this model on Heroku. Since Heroku only supports scikit-learn models, we were unable to use PySpark or Tensorflow. However, we found that NLTK offers the same functionality and would work with Heroku. NLTK is a platform for building Python programs to work with human language data. It contains a suite of text processing libraries for classification, tokenization, lemmatization, etc.</p>
            <p class="mb-30">We built a processing pipeline with the following steps:</p>
            <ol type="1">
              <li><strong>Removing Punctuation</strong> - Since we were dealing with tweets, there were a host of punctuation marks we chose to eliminate. In today’s language, emoticons such as ‘:D’ could actually yield some sort of sentiment. However, as there were many other unnecessary punctuation marks, we chose to eliminate all of them.</li>
              <li><strong>Tokenizing</strong> - This key step in the pipeline separates words so that each can be analyzed individually, usually by whitespace but in certain situations, a contraction such as I’m needs to be separated into the two words I am.</li>
              <li><strong>Removing Stop Words</strong> - Stop words are common words that hold no sentiment and are thus unnecessary in training the model. Examples of stop words include: ‘a,’ ‘and,’ ‘or.’ </li>
              <li><strong>Lemmatization</strong> - This step identifies when two words have the same root and maps them to one word. For example: ‘code,’ ‘coding,’ ‘codes,’ and ‘coded’ will all get mapped to ‘code.’</li>
              <li><strong>Vectorization</strong> - This final step maps each word to a numerical value.</li>
            </ol>  

            <p>The preprocessing took quite a bit of time. In total, it took almost three hours to run our data through steps 1-4 of the pipeline. To avoid doing these steps again, we saved the preprocessed data so that we could easily call it later. Then began training our models. At this step, we vectorized the data, which maps each word to a numerical value, and split the data into a training set and a testing set.</p>
            <p>We chose four different types of models. However, they all had a few things in common that made them ideal choices for our project. They all did well with text classification in general and they were all very fast. Since our dataset consisted of 1.6 million tweets, speed was important. The first model we chose was the Multinomial Naive Bayes Classifier (NBC). Initially, we obtained a score of about 76.5% on the test set. Then, we chose the Stochastic Gradient Descent (SGD) model which scored about 76%. Next, we chose the Logistic Regression Classifier (LRC) which scored about 78.3%. Finally, we tried the Light Gradient Boosting (LGB) model and obtained a score of about 66%. After performing a grid search on each model,  we were able to increase the testing score of the SGD model to about 78.4% by adjusting the alpha parameter. This gave it a slight edge over the LRC model, currently our best performer. We were unable to increase the scores for any of the other models.</p>
            <p>Next we saved our vectorizer and each model so that they could be deployed to Heroku. We began with our two best-performing models (SGD and LRC) keeping in mind the size restrictions that Heroku imposes. The fact that these models performed best makes sense. Even considering our huge dataset, the zero-frequency problem is still an issue with the NBC model and the excess noise is still an issue for the LGB model. However, after finding out that the models were rather small and we had plenty of space to work with, we decided to include them all.</p>

            <p class = "mb-30"> Finally, we started making some predictions. We ran into <b><u>three problems</u></b>:</p>
            <ol type="1">
              <li>The probabilities wouldn’t display for the SGD model. We found that the ‘predict_proba’ attribute couldn’t be used when we used ‘hinge’ for the ‘loss’ parameter. Although this was determined by our grid search to be the best parameter, we actually didn’t lose any accuracy by changing it to ‘log.’ This cleared up that error.</li>
              <li>For some reason, the displayed percentages wouldn’t round properly in certain fringe cases. We were unable to figure out why, but we were able to convert each value in the float to a string, then select the first five values. For example 29.45 would be the first five values for 29.45348576. This affects the rounding of the hundredths place in some cases but we could always include more decimal places if need be. The important part was getting the values to display in a consistent way.</li>
              <li>When we input a tweet that was completely removed in the preprocessing pipeline, it caused the prediction to fail. For example, since we removed all punctuation, a tweet such as ‘:D’ would be completely removed. Similarly, a list of only stop words such as ‘and or a’ would be completely removed. This meant that there was nothing left to vectorize which broke the prediction. To remedy this, we used Try/Except around the vectorizer which prompted the user to enter a new tweet if such a situation occurred.</li>
            </ol> 
          </div>
                </div>
              </form>
            </div>
          </div>
        </div>              
      </div>
      <!--== End Post Details Content ==-->

  <!--== Start Footer Area Wrapper ==-->
  <footer class="footer-area reveal-footer border-top-style">
    <div class="container-fluid">
      <div class="row">
        <div class="col-sm-12">
          <div class="footer-content" id = "parentDiv">
            <div class="widget-item text-center" id = "mainHeader">
              <div class="widget-copyright">
                <p>© 2022 <span>Machine Learning Tweet Sentiment Predictor</span></p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <!--== End Footer Area Wrapper ==-->
  
  <!--== Start Side Menu ==-->
  <aside class="off-canvas-wrapper">
    <div class="off-canvas-inner">
      <div class="off-canvas-overlay"></div>
      <!-- Start Off Canvas Content Wrapper -->
      <div class="off-canvas-content">
        <!-- Off Canvas Header -->
        <div class="off-canvas-header">
          <div class="close-action">
            <button class="btn-close"><i class="icon_close"></i></button>
          </div>
        </div>

        <div class="off-canvas-item">
          <!-- Start Asside Menu Wrapper -->
          <div class="asside-navigation-area">
            <ul class="asside-menu nav" id="assideMenuNav">
              <li><a href="{{ url_for('home') }}">Home</a></li>
              <li><a href="{{ url_for('team') }}">About</a></li>
              <li><a href="{{ url_for('word') }}">Word Collage</a></li>
              <li><a href="#"> Summary Page </a></li>
            </ul>
          </div>
          <!-- End Asside Menu Wrapper -->
        </div>
        <!-- Off Canvas Footer -->
        <div class="off-canvas-footer"></div>
      </div>
      <!-- End Off Canvas Content Wrapper -->
    </div>
  </aside>
  <!--== End Side Menu ==-->
</div>

<!--=======================Javascript============================-->

<!--=== Modernizr Min Js ===-->
<script src="static/js/modernizr.js"></script>
<!--=== jQuery Min Js ===-->
<script src="static/js/jquery-main.js"></script>
<!--=== jQuery Migration Min Js ===-->
<script src="static/js/jquery-migrate.js"></script>
<!--=== Popper Min Js ===-->
<script src="static/js/popper.min.js"></script>
<!--=== Bootstrap Min Js ===-->
<script src="static/js/bootstrap.min.js"></script>
<!--=== jquery UI Min Js ===-->
<script src="static/js/jquery-ui.min.js"></script>
<!--=== Plugin Collection Js ===-->
<script src="static/js/plugincollection.js"></script>

<!--=== Custom Js ===-->
<script src="static/js/custom.js"></script>

</body>

</html>