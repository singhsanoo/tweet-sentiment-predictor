<!DOCTYPE html>
<html>
  <head>
    <!-- These three elements must come first! -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <!-- CSS files will be here -->
    <link rel="stylesheet" href="static/style.css" />
    <link rel="stylesheet" href="static/reset.css" />
    <link rel="stylesheet" href="static/bootstap.min.css" media="screen" />
    <!-- Import Bootstsrap -->
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3"
      crossorigin="anonymous"
    />

    <!-- Give it a title -->
    <title>Tweet Sentiment Predictor</title>
  </head>

  <body>
    <header>
      <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <div class="container-fluid">
          <button
            class="navbar-toggler"
            type="button"
            data-bs-toggle="collapse"
            data-bs-target="#navbarTogglerDemo02"
            aria-controls="navbarTogglerDemo02"
            aria-expanded="false"
            aria-label="Toggle navigation"
          >
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="collapse navbar-collapse" id="navbarTogglerDemo02">
            <ul class="navbar-nav me-auto mb-2 mb-lg-0">
              <li class="nav-item">
                <a class="nav-link" href="{{ url_for('home') }}"
                  >Tweet Sentiment Predictor</a
                >
              </li>
              <li class="nav-item">
                <a class="nav-link" href="{{ url_for('word') }}"
                  >Word Collage</a
                >
              </li>
              <li class="nav-item">
                <a class="nav-link active" aria-current="page" href="#"
                  >Summary</a
                >
              </li>
              <li class="nav-item">
                <a class="nav-link" href="{{ url_for('team') }}">Team</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
      <div class="two-toned-header-color"></div>
    </header>


    <!-- ================================================================ -->
    <style>
      body {
        background-image: url(static/images/twitter_logos_top_r_corner.png);
        background-repeat: no-repeat;
        background-size: cover;
      }
    </style>

    <!-- Summary Starts Here -->
    <div class="content-container">
      <h1 class="text-center">Model Descriptions</h1>
      <div class="bg-light my-3 mx-3 p-3" style="border-radius: 15px">
        <p>
          <b>Stochastic Gradient Descent:</b> Gradient descent is an
          optimization algorithm that attempts to minimize a cost function.
          However, each iteration of gradient descent runs through the entire
          datasets searching for an optimal solution. Stochastic gradient
          descent makes this process a bit faster. It shuffles the data to avoid
          any pre-existing patterns, divides it into a finite number of subsets.
          Each iteration is then performed on an individual subset rather than
          the entire dataset. This makes regular gradient descent but can result
          in a higher variance.
        </p>
      </div>

      <div class="bg-light my-3 mx-3 p-3" style="border-radius: 15px">
        <p>
          <b>Light Gradient Boosting:</b> Gradient boosting is a model based on
          decision trees. However, unlike Random Forests where trees are built
          independently, gradient boosted trees are built additively. That is,
          they correct the errors of the trees that came before. This leads to
          faster training times, lower memory usage, and higher accuracy working
          with complex data. This makes LGB ideal for very large data sets.
          However, they are far more susceptible to noise.
        </p>
      </div>

      <div class="bg-light my-3 mx-3 p-3" style="border-radius: 15px">
        <p>
          <b>Logistic Regression:</b> Logistic regression is an algorithm that
          classifies data by considering extreme outcomes and tries to make a
          logarithmic line that distinguishes between them. Logistic models are
          simple and provide great efficiency. However, they’re not as flexible
          as other models so it’s difficult for them to capture more complex
          relationships.
        </p>
      </div>

      <div class="bg-light my-3 mx-3 p-3" style="border-radius: 15px">
        <p>
          <b>Multinomial Naive Bayes:</b> Naive Bayes classifier makes use of
          Bayes’s Theorem which describes the relationship between conditional
          probabilities of certain quantities. This lets us find the probability
          of a label given some observed features. Multinomial Naive Bayes takes
          this a step further by analyzing how often a given term is
          represented. This model is well-suited for large data sets because it
          is so fast. The biggest downfall of this model is known as the
          zero-frequency problem. That is, it assigns a probability of zero to a
          categorical value that is present in the testing set but wasn’t
          available in the training set.
        </p>
      </div>

      <h1 class="text-center">Project Summary</h1>
      <div class="bg-light my-3 mx-3 p-3" style="border-radius: 15px">
        <p>
          The goal of this project was to see if it was possible to train a
          machine learning model to recognize the sentiment of a tweet. We began
          by discussing possible limitations. In the end, we wanted to host this
          model on Heroku. Since Heroku only supports scikit-learn models, we
          were unable to use PySpark or Tensorflow. However, we found that NLTK
          offers the same functionality and would work with Heroku. NLTK is a
          platform for building Python programs to work with human language
          data. It contains a suite of text processing libraries for
          classification, tokenization, lemmatization, etc.
        </p>
        <p>We built a processing pipeline with the following steps:</p>
        <ol>
          <li>
            <b>Removing Punctuation -</b> Since we were dealing with tweets,
            there were a host of punctuation marks we chose to eliminate. In
            today’s language, emoticons such as ‘:D’ could actually yield some
            sort of sentiment. However, as there were many other unnecessary
            punctuation marks, we chose to eliminate all of them.
          </li>
          <li>
            <b>Tokenizing -</b> This key step in the pipeline separates words so
            that each can be analyzed individually, usually by whitespace but in
            certain situations, a contraction such as I’m needs to be separated
            into the two words I am.
          </li>
          <li>
            <b>Removing Stop Words -</b> Stop words are common words that hold
            no sentiment and are thus unnecessary in training the model.
            Examples of stop words include: ‘a,’ ‘and,’ ‘or.’
          </li>
          <li>
            <b>Lemmatization -</b> This step identifies when two words have the
            same root and maps them to one word. For example: ‘code,’ ‘coding,’
            ‘codes,’ and ‘coded’ will all get mapped to ‘code.’
          </li>
          <li>
            <b>Vectorization -</b> This final step maps each word to a numerical
            value
          </li>
        </ol>
        <p>
          The preprocessing took quite a bit of time. In total, it took almost
          three hours to run our data through steps 1-4 of the pipeline. To
          avoid doing these steps again, we saved the preprocessed data so that
          we could easily call it later. Then began training our models. At this
          step, we vectorized the data, which maps each word to a numerical
          value, and split the data into a training set and a testing set.
        </p>
        <p>
          We chose four different types of models. However, they all had a few
          things in common that made them ideal choices for our project. They
          all did well with text classification in general and they were all
          very fast. Since our dataset consisted of 1.6 million tweets, speed
          was important. The first model we chose was the Multinomial Naive
          Bayes Classifier (NBC). Initially, we obtained a score of about 76.5%
          on the test set. Then, we chose the Stochastic Gradient Descent (SGD)
          model which scored about 76%. Next, we chose the Logistic Regression
          Classifier (LRC) which scored about 78.3%. Finally, we tried the Light
          Gradient Boosting (LGB) model and obtained a score of about 66%. After
          performing a grid search on each model, we were able to increase the
          testing score of the SGD model to about 78.4% by adjusting the alpha
          parameter. This gave it a slight edge over the LRC model, currently
          our best performer. We were unable to increase the scores for any of
          the other models.
        </p>
        <p>
          Next we saved our vectorizer and each model so that they could be
          deployed to Heroku. We began with our two best-performing models (SGD
          and LRC) keeping in mind the size restrictions that Heroku imposes.
          The fact that these models performed best makes sense. Even
          considering our huge dataset, the zero-frequency problem is still an
          issue with the NBC model and the excess noise is still an issue for
          the LGB model. However, after finding out that the models were rather
          small and we had plenty of space to work with, we decided to include
          them all.
        </p>
        <p>
          Finally, we started making some predictions. We ran into three
          problems:
        </p>
        <ol>
          <li>
            The probabilities wouldn’t display for the SGD model. We found that
            the ‘predict_proba’ attribute couldn’t be used when we used ‘hinge’
            for the ‘loss’ parameter. Although this was determined by our grid
            search to be the best parameter, we actually didn’t lose any
            accuracy by changing it to ‘log.’ This cleared up that error.
          </li>
          <li>
            For some reason, the displayed percentages wouldn’t round properly
            in certain fringe cases. We were unable to figure out why, but we
            were able to convert each value in the float to a string, then
            select the first five values. For example 29.45 would be the first
            five values for 29.45348576. This affects the rounding of the
            hundredths place in some cases but we could always include more
            decimal places if need be. The important part was getting the values
            to display in a consistent way.
          </li>
          <li>
            When we input a tweet that was completely removed in the
            preprocessing pipeline, it caused the prediction to fail. For
            example, since we removed all punctuation, a tweet such as ‘:D’
            would be completely removed. Similarly, a list of only stop words
            such as ‘and or a’ would be completely removed. This meant that
            there was nothing left to vectorize which broke the prediction. To
            remedy this, we used Try/Except around the vectorizer which prompted
            the user to enter a new tweet if such a situation occurred.
          </li>
        </ol>
      </div>
    </div>

    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
      crossorigin="anonymous"
    ></script>

    <!-- jQuery CDN -->
    <script
      type="text/javascript"
      hef="#"
      src="https://code.jquery.com/jquery-2.1.4.min.js"
    ></script>

    <!-- Bootstrap CDN -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

    <!-- Start footer -->
    <footer class="footer-bottom navbar-fixed-bottom" id="footer-index">
      <div class="two-toned-footer-color"></div>
      <p class="text-muted text-muted-footer text-center">
        Machine Learning Tweet Sentiment Predictor
      </p>
    </footer>
  </body>
</html>
